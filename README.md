# A Survey on State-of-the-Art Attention Mechanisms

### Tasks:

- [X] Translate Notebook into Repo
- [ ] Notebook transformer block
- [ ] Model sharding -> /sharding branch
- [ ] Rename module to attention
- [ ] Make `__init__.py` imports better
- [ ] Dataset
- [ ] Evals
- [ ] Separate into flax and torch
- [ ] Append kernels into modules / or separate module
- [ ] Package it to PyPi
